# Obsidian 2025 åŠŸèƒ½ä»£ç å®ç°æ–‡æ¡£

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£åŒ…å« Obsidian 2025 åŠŸèƒ½åˆ†ææŠ¥å‘Šä¸­æ‰€æœ‰æŠ€æœ¯å®ç°ä»£ç ã€‚ä¸»æŠ¥å‘Šæ–‡æ¡£ä¸“æ³¨äºåŠŸèƒ½åˆ†æå’ŒæŠ€æœ¯æ¶æ„æè¿°ï¼Œè€Œæœ¬æ–‡æ¡£æä¾›å®Œæ•´çš„ä»£ç å®ç°ç»†èŠ‚ã€‚

---

## ğŸš€ AIåŸç”Ÿé›†æˆæŠ€æœ¯å®ç°

### 1. AIé›†æˆæ¶æ„èŒƒå¼

```typescript
// 2025å¹´AIé›†æˆæ¶æ„èŒƒå¼
interface AINativeArchitecture2025 {
  // ç»Ÿä¸€AIç½‘å…³
  aiGateway: AIGateway;
  // å¤šæ¨¡æ€å¤„ç†å™¨
  multimodalProcessor: MultimodalProcessor;
  // ä¸Šä¸‹æ–‡ç®¡ç†å™¨
  contextManager: ContextManager;
  // å®æ—¶æ¨ç†å¼•æ“
  realTimeInference: RealTimeInferenceEngine;
  // ä¸ªæ€§åŒ–é€‚é…å™¨
  personalizationAdapter: PersonalizationAdapter;
}
```

### 2. å®æ—¶æ™ºèƒ½å»ºè®®å¼•æ“

```python
class RealTimeSuggestionEngine:
    def __init__(self):
        self.context_window = 1000  # å­—ç¬¦ä¸Šä¸‹æ–‡çª—å£
        self.suggestion_cache = LRUCache(1000)  # å»ºè®®ç¼“å­˜
        
    async def get_suggestions(self, current_content: str, cursor_position: int):
        # æå–å½“å‰ç¼–è¾‘ä¸Šä¸‹æ–‡
        context = self.extract_editing_context(current_content, cursor_position)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self.generate_cache_key(context)
        if cached := self.suggestion_cache.get(cache_key):
            return cached
        
        # AIæ¨ç†ç”Ÿæˆå»ºè®®
        suggestions = await self.ai_inference(context)
        
        # ç¼“å­˜ç»“æœ
        self.suggestion_cache.put(cache_key, suggestions)
        return suggestions
```

---

## ğŸ† çƒ­é—¨æ’ä»¶æŠ€æœ¯å®ç°

### 1. Smart Connections 2.0 æŠ€æœ¯æ¶æ„

```typescript
interface SmartConnectionsArchitecture {
  // æœ¬åœ°å‘é‡å­˜å‚¨
  vectorStore: LocalVectorDB;
  // è¯­ä¹‰æœç´¢å¼•æ“
  semanticSearchEngine: SemanticSearch;
  // å®æ—¶ç´¢å¼•å™¨
  realTimeIndexer: IndexingService;
  // æ¨èç®—æ³•
  recommendationEngine: RecommendationSystem;
}
```

### 2. CoPilot for Obsidian - Vault Q&A ç³»ç»Ÿ

```typescript
// å®Œæ•´çš„Vault Q&Aç³»ç»Ÿå®ç°
class VaultQASystem {
  private indexManager: IndexManager;
  private retrievalEngine: RetrievalEngine;
  private answerGenerator: AnswerGenerator;
  private cacheManager: CacheManager;
  
  constructor() {
    this.indexManager = new IndexManager();
    this.retrievalEngine = new HybridRetrievalEngine();
    this.answerGenerator = new LLMAnswerGenerator();
    this.cacheManager = new LRUCache(1000);
  }
  
  async indexEntireVault(vaultPath: string): Promise<IndexStats> {
    console.log('å¼€å§‹ç´¢å¼•çŸ¥è¯†åº“:', vaultPath);
    
    // 1. æ‰«ææ‰€æœ‰æ–‡æ¡£
    const documents = await this.scanDocuments(vaultPath);
    
    // 2. æ–‡æ¡£é¢„å¤„ç†
    const processedDocs = await this.preprocessDocuments(documents);
    
    // 3. ç”ŸæˆåµŒå…¥å‘é‡
    const embeddings = await this.generateEmbeddings(processedDocs);
    
    // 4. æ„å»ºå‘é‡ç´¢å¼•
    const index = await this.indexManager.buildIndex(embeddings);
    
    // 5. å­˜å‚¨å…ƒæ•°æ®
    await this.storeMetadata(processedDocs, embeddings);
    
    console.log('çŸ¥è¯†åº“ç´¢å¼•å®Œæˆï¼Œå…±å¤„ç†æ–‡æ¡£:', documents.length);
    return {
      totalDocuments: documents.length,
      totalEmbeddings: embeddings.length,
      indexSize: await this.getIndexSize(),
      processingTime: Date.now() - startTime
    };
  }
  
  async answerQuestion(question: string, options: QAOptions = {}): Promise<Answer> {
    const cacheKey = this.generateCacheKey(question, options);
    
    // æ£€æŸ¥ç¼“å­˜
    if (options.useCache !== false) {
      const cached = this.cacheManager.get(cacheKey);
      if (cached) {
        console.log('å‘½ä¸­ç¼“å­˜:', cacheKey);
        return cached;
      }
    }
    
    // 1. æŸ¥è¯¢é‡å†™å’Œæ‰©å±•
    const expandedQueries = await this.queryExpander.expandQuery(question);
    
    // 2. å¤šè½®æ£€ç´¢
    const retrievalResults = await Promise.all(
      expandedQueries.map(query => 
        this.retrievalEngine.retrieve(query, {
          limit: options.limit || 10,
          scoreThreshold: options.scoreThreshold || 0.7
        })
      )
    );
    
    // 3. ç»“æœèåˆå’Œé‡æ’åº
    const mergedResults = this.mergeRetrievalResults(retrievalResults);
    const rerankedResults = await this.rerankResults(mergedResults, question);
    
    // 4. ä¸Šä¸‹æ–‡æ„å»º
    const context = this.buildContext(rerankedResults, question);
    
    // 5. ç”Ÿæˆå›ç­”
    const answer = await this.answerGenerator.generateAnswer(question, context, {
      maxLength: options.maxLength,
      temperature: options.temperature,
      includeSources: options.includeSources !== false
    });
    
    // 6. ç¼“å­˜ç»“æœ
    this.cacheManager.set(cacheKey, answer, { ttl: 3600000 }); // 1å°æ—¶ç¼“å­˜
    
    return answer;
  }
  
  // é«˜çº§æ£€ç´¢åŠŸèƒ½
  async advancedRetrieval(query: string, filters: RetrievalFilters = {}): Promise<RetrievalResult[]> {
    const results = await this.retrievalEngine.advancedRetrieve(query, {
      ...filters,
      // æ—¶é—´è¿‡æ»¤å™¨
      timeRange: filters.timeRange || { from: null, to: null },
      // æ–‡æ¡£ç±»å‹è¿‡æ»¤å™¨
      documentTypes: filters.documentTypes || ['md', 'txt', 'pdf'],
      // æ ‡ç­¾è¿‡æ»¤å™¨
      tags: filters.tags || [],
      // ç›¸å…³æ€§é˜ˆå€¼
      scoreThreshold: filters.scoreThreshold || 0.6,
      // æœ€å¤§ç»“æœæ•°
      limit: filters.limit || 20
    });
    
    return results;
  }
}

// æ£€ç´¢ç»“æœç±»å‹å®šä¹‰
interface RetrievalResult {
  documentId: string;
  documentTitle: string;
  content: string;
  score: number;
  metadata: {
    documentType: string;
    lastModified: Date;
    tags: string[];
    wordCount: number;
  };
  highlights: {
    text: string;
    score: number;
    position: number;
  }[];
}

// å›ç­”ç”Ÿæˆé€‰é¡¹
interface QAOptions {
  useCache?: boolean;
  maxLength?: number;
  temperature?: number;
  includeSources?: boolean;
  limit?: number;
  scoreThreshold?: number;
}

// é«˜çº§æ£€ç´¢è¿‡æ»¤å™¨
interface RetrievalFilters {
  timeRange?: { from: Date | null; to: Date | null };
  documentTypes?: string[];
  tags?: string[];
  scoreThreshold?: number;
  limit?: number;
}
```

### 3. Supabase åç«¯é›†æˆ

```typescript
// Supabaseé›†æˆç¤ºä¾‹
const supabaseIntegration = {
  realtime: true, // å®æ—¶æ•°æ®åŒæ­¥
  rowLevelSecurity: true, // è¡Œçº§å®‰å…¨
  postgresExtensions: ['vector', 'pg_trgm', 'pg_search'], // PostgreSQLæ‰©å±•
  storage: {
    encrypted: true, // åŠ å¯†å­˜å‚¨
    compression: 'zstd', // é«˜æ•ˆå‹ç¼©
    versioning: true // ç‰ˆæœ¬æ§åˆ¶
  }
};
```

### 4. OpenAI é›†æˆä¼˜åŒ–

```python
# æ™ºèƒ½APIä½¿ç”¨ä¼˜åŒ–
class OpenAIOptimizer:
    def __init__(self):
        self.tokenCounter = TokenCounter()
        self.cache = RedisCache()
        self.rateLimiter = RateLimiter()
    
    async def optimized_completion(self, prompt, max_tokens=1000):
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self.generate_cache_key(prompt)
        if cached := self.cache.get(cache_key):
            return cached
        
        # ä»¤ç‰Œä¼˜åŒ–
        optimized_prompt = self.optimize_prompt_length(prompt, max_tokens)
        
        # é€Ÿç‡é™åˆ¶
        await self.rateLimiter.wait_if_needed()
        
        # è°ƒç”¨API
        response = await openai.Completion.create(
            model="gpt-4",
            prompt=optimized_prompt,
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        # ç¼“å­˜ç»“æœ
        self.cache.set(cache_key, response, expire=3600)
        
        return response
```

### 5. åˆ†å±‚ç¼“å­˜ç³»ç»Ÿ

```typescript
interface LayeredCacheSystem {
  // å†…å­˜ç¼“å­˜ - æ¯«ç§’çº§å“åº”
  memoryCache: LRUCache<string, any>;
  // Redisç¼“å­˜ - ç§’çº§å“åº”  
  redisCache: RedisClient;
  // ç£ç›˜ç¼“å­˜ - åˆ†é’Ÿçº§æŒä¹…åŒ–
  diskCache: FileSystemCache;
  
  async get(key: string): Promise<any> {
    // 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
    if (memoryCache.has(key)) return memoryCache.get(key);
    
    // 2. æ£€æŸ¥Redisç¼“å­˜
    const redisValue = await redisCache.get(key);
    if (redisValue) {
      // å›å¡«å†…å­˜ç¼“å­˜
      memoryCache.set(key, redisValue);
      return redisValue;
    }
    
    // 3. æ£€æŸ¥ç£ç›˜ç¼“å­˜
    const diskValue = await diskCache.get(key);
    if (diskValue) {
      // å›å¡«Rediså’Œå†…å­˜ç¼“å­˜
      await redisCache.set(key, diskValue);
      memoryCache.set(key, diskValue);
      return diskValue;
    }
    
    return null;
  }
}
```

### 6. å¢é‡ç´¢å¼•æ›´æ–°

```python
class IncrementalIndexer:
    def __init__(self):
        self.watcher = FileSystemWatcher()
        self.changeProcessor = ChangeProcessor()
        
    async def start_watching(self, vault_path):
        # ç›‘å¬æ–‡ä»¶ç³»ç»Ÿå˜åŒ–
        self.watcher.watch(vault_path, {
            'persistent': True,
            'recursive': True,
            'ignoreInitial': False
        })
        
        self.watcher.on('change', async (path, stats) => {
            # å¤„ç†æ–‡ä»¶å˜åŒ–
            await this.handleFileChange(path, stats);
        })
    
    async def handleFileChange(self, path, stats):
        if (stats.event === 'change' || stats.event === 'add') {
            // å¢é‡æ›´æ–°ç´¢å¼•
            await this.updateIndexForFile(path);
        } else if (stats.event === 'unlink') {
            // ä»ç´¢å¼•ä¸­ç§»é™¤
            await this.removeFromIndex(path);
        }
```

### 7. Text Generator Pro å¤šæ¨¡å‹æ¶æ„

```typescript
class MultiModelTextGenerator {
  private providers: AIProvider[] = [
    new OpenAIProvider(),
    new AnthropicProvider(), 
    new GoogleProvider(),
    new LocalModelProvider()
  ];
  
  async generateContent(prompt: string, modelType: string) {
    const provider = this.getProvider(modelType);
    return await provider.generate(prompt);
  }
}
```

### 8. Khoj å®Œå…¨ç¦»çº¿æ¶æ„

```python
class FullyOfflineAI {
  def __init__(self):
    self.localLLM = loadLocalModel()
    self.vectorDB = setupLocalVectorDB()
    
  def processDocument(self, content: str):
    # æœ¬åœ°å¤„ç†ï¼Œæ— éœ€ç½‘ç»œ
    embedding = self.localLLM.embed(content)
    self.vectorDB.store(embedding)
```

### 9. Omnisearch Pro æ··åˆæœç´¢ç®—æ³•

```typescript
class HybridSearchEngine {
  async search(query: string): Promise<SearchResult[]> {
    const keywordResults = await keywordSearch(query);
    const semanticResults = await semanticSearch(query);
    
    // æ™ºèƒ½ç»“æœèåˆ
    return this.mergeResults(keywordResults, semanticResults, {
      keywordWeight: 0.4,
      semanticWeight: 0.6,
      recencyBias: 0.1
    });
  }
}
```

### 10. Note Linker AI è‡ªåŠ¨é“¾æ¥å‘ç°

```typescript
class AutoLinkDiscoverer {
  async discoverLinks(content: string, allNotes: Note[]) {
    const entities = await extractEntities(content);
    const potentialLinks = [];
    
    for (const entity of entities) {
      const relatedNotes = await findRelatedNotes(entity, allNotes);
      potentialLinks.push(...relatedNotes.map(note => ({
        source: currentNoteId,
        target: note.id,
        confidence: calculateConfidence(entity, note)
      })));
    }
    
    return potentialLinks.filter(link => link.confidence > 0.7);
  }
}
```

---

## ğŸ’¡ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. ä¼ä¸šçº§è¯­ä¹‰æœç´¢ç³»ç»Ÿ

```typescript
// ä¼ä¸šçº§è¯­ä¹‰æœç´¢ç³»ç»Ÿæ¶æ„
class EnterpriseSemanticSearchSystem {
  // æ ¸å¿ƒç»„ä»¶
  private embeddingService: EmbeddingService;
  private vectorDB: VectorDatabase;
  private queryProcessor: QueryProcessor;
  private rankingEngine: RankingEngine;
  private cacheManager: CacheManager;
  private monitoring: MonitoringService;
  
  // é…ç½®é€‰é¡¹
  private config: SearchConfig = {
    maxResults: 50,
    minScoreThreshold: 0.6,
    timeoutMs: 5000,
    fallbackToKeyword: true,
    hybridSearchRatio: 0.7 // 70%è¯­ä¹‰ + 30%å…³é”®è¯
  };
  
  constructor(options?: Partial<SearchConfig>) {
    this.config = { ...this.config, ...options };
    this.initializeComponents();
  }
  
  private initializeComponents(): void {
    // åˆå§‹åŒ–åµŒå…¥æœåŠ¡
    this.embeddingService = new EmbeddingService({
      model: 'all-mpnet-base-v2', // å¹³è¡¡æ€§èƒ½å’Œè´¨é‡
      batchSize: 32,
      maxSequenceLength: 512
    });
    
    // åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    this.vectorDB = new ChromaDB({
      path: './data/vector_store',
      collectionName: 'documents',
      similarityMetric: 'cosine'
    });
    
    // æŸ¥è¯¢å¤„ç†å™¨
    this.queryProcessor = new AdvancedQueryProcessor({
      queryExpansion: true,
      spellCheck: true,
      synonymMapping: true,
      stopwordRemoval: true
    });
    
    // æ’åºå¼•æ“
    this.rankingEngine = new HybridRankingEngine({
      semanticWeight: 0.7,
      keywordWeight: 0.2,
      recencyWeight: 0.05,
      popularityWeight: 0.05
    });
    
    // ç¼“å­˜ç®¡ç†
    this.cacheManager = new LayeredCache({
      memory: { maxSize: 1000, ttl: 300000 },
      redis: { host: 'localhost', port: 6379, ttl: 3600000 },
      disk: { path: './cache', ttl: 86400000 }
    });
    
    // ç›‘æ§æœåŠ¡
    this.monitoring = new PrometheusMonitoring({
      metricsPrefix: 'semantic_search_',
      collectInterval: 30000
    });
  }
  
  // æ–‡æ¡£ç´¢å¼•æ–¹æ³•
  async indexDocument(document: SearchDocument): Promise<IndexResult> {
    const startTime = Date.now();
    
    try {
      // 1. æ–‡æœ¬é¢„å¤„ç†
      const processedText = await this.preprocessText(document.content);
      
      // 2. ç”ŸæˆåµŒå…¥å‘é‡
      const embedding = await this.embeddingService.embed(processedText);
      
      // 3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
      const documentId = await this.vectorDB.upsert({
        id: document.id,
        embedding: embedding.vector,
        metadata: {
          title: document.title,
          contentType: document.type,
          language: document.language,
          wordCount: document.content.length,
          createdAt: new Date(),
          updatedAt: new Date()
        }
      });
      
      // 4. æ›´æ–°å€’æ’ç´¢å¼•ï¼ˆç”¨äºå…³é”®è¯æœç´¢ï¼‰
      await this.updateInvertedIndex(document);
      
      const duration = Date.now() - startTime;
      this.monitoring.recordIndexingTime(duration);
      
      return { success: true, documentId, processingTime: duration };
      
    } catch (error) {
      this.monitoring.recordError('indexing_error');
      throw new Error(`æ–‡æ¡£ç´¢å¼•å¤±è´¥: ${error.message}`);
    }
  }
  
  // æœç´¢æ–¹æ³•
  async search(query: string, options?: SearchOptions): Promise<SearchResults> {
    const searchId = this.generateSearchId();
    const startTime = Date.now();
    
    try {
      // æ£€æŸ¥ç¼“å­˜
      const cacheKey = this.generateCacheKey(query, options);
      const cachedResults = await this.cacheManager.get(cacheKey);
      
      if (cachedResults) {
        this.monitoring.recordCacheHit();
        return {
          ...cachedResults,
          cached: true,
          searchId
        };
      }
      
      this.monitoring.recordCacheMiss();
      
      // 1. æŸ¥è¯¢é¢„å¤„ç†
      const processedQuery = await this.queryProcessor.process(query);
      
      // 2. ç”ŸæˆæŸ¥è¯¢åµŒå…¥
      const queryEmbedding = await this.embeddingService.embed(processedQuery);
      
      // 3. è¯­ä¹‰æœç´¢
      const semanticResults = await this.vectorDB.search({
        queryEmbedding: queryEmbedding.vector,
        limit: options?.limit || this.config.maxResults,
        minScore: options?.minScore || this.config.minScoreThreshold
      });
      
      // 4. å…³é”®è¯æœç´¢ï¼ˆæ··åˆæœç´¢ï¼‰
      let keywordResults: SearchResult[] = [];
      if (this.config.fallbackToKeyword) {
        keywordResults = await this.keywordSearch(processedQuery, {
          limit: Math.floor(this.config.maxResults * 0.3) // 30%çš„ç»“æœæ¥è‡ªå…³é”®è¯
        });
      }
      
      // 5. ç»“æœèåˆå’Œé‡æ’åº
      const allResults = this.mergeResults(semanticResults, keywordResults);
      const rankedResults = await this.rankingEngine.rank(allResults, {
        query,
        userPreferences: options?.userPreferences
      });
      
      // 6. åå¤„ç†
      const finalResults = await this.postProcessResults(rankedResults, query);
      
      // 7. ç¼“å­˜ç»“æœ
      await this.cacheManager.set(cacheKey, {
        results: finalResults,
        query,
        timestamp: new Date()
      }, { ttl: this.calculateCacheTTL(finalResults) });
      
      const duration = Date.now() - startTime;
      this.monitoring.recordSearchTime(duration);
      
      return {
        results: finalResults,
        totalCount: finalResults.length,
        searchId,
        processingTime: duration,
        cached: false
      };
      
    } catch (error) {
      this.monitoring.recordError('search_error');
      
      // é™çº§åˆ°å…³é”®è¯æœç´¢
      if (this.config.fallbackToKeyword) {
        console.warn('è¯­ä¹‰æœç´¢å¤±è´¥ï¼Œé™çº§åˆ°å…³é”®è¯æœç´¢:', error.message);
        return await this.fallbackKeywordSearch(query, options);
      }
      
      throw new Error(`æœç´¢å¤±è´¥: ${error.message}`);
    }
  }
  
  // æ‰¹é‡ç´¢å¼•æ–¹æ³•
  async batchIndex(documents: SearchDocument[]): Promise<BatchIndexResult> {
    const results: BatchIndexResult = {
      successful: [],
      failed: [],
      total: documents.length
    };
    
    // åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
    const batchSize = 100;
    for (let i = 0; i < documents.length; i += batchSize) {
      const batch = documents.slice(i, i + batchSize);
      
      try {
        const batchResults = await Promise.allSettled(
          batch.map(doc => this.indexDocument(doc))
        );
        
        batchResults.forEach((result, index) => {
          const doc = batch[index];
          if (result.status === 'fulfilled') {
            results.successful.push({
              documentId: doc.id,
              result: result.value
            });
          } else {
            results.failed.push({
              documentId: doc.id,
              error: result.reason.message
            });
          }
        });
        
        // çŸ­æš‚çš„å»¶è¿Ÿä»¥é¿å…è¿‡åº¦è´Ÿè½½
        await this.delay(100);
        
      } catch (batchError) {
        console.error('æ‰¹å¤„ç†å¤±è´¥:', batchError);
        batch.forEach(doc => {
          results.failed.push({
            documentId: doc.id,
            error: batchError.message
          });
        });
      }
    }
    
    return results;
  }
  
  // ç³»ç»Ÿç›‘æ§æ–¹æ³•
  getSystemMetrics(): SystemMetrics {
    return {
      cacheStats: this.cacheManager.getStats(),
      vectorDBStats: this.vectorDB.getStats(),
      embeddingStats: this.embeddingService.getStats(),
      performanceMetrics: this.monitoring.getMetrics()
    };
  }
  
  // ç§æœ‰å·¥å…·æ–¹æ³•
  private async preprocessText(text: string): Promise<string> {
    // æ–‡æœ¬æ¸…ç†ã€æ ‡å‡†åŒ–ã€åˆ†è¯ç­‰
    return text
      .toLowerCase()
      .replace(/[^\w\s]/g, ' ') // ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦
      .replace(/\s+/g, ' ')     // åˆå¹¶å¤šä½™ç©ºæ ¼
      .trim();
  }
  
  private generateSearchId(): string {
    return `search_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }
  
  private generateCacheKey(query: string, options?: any): string {
    const optionsHash = options ? this.hashObject(options) : 'default';
    return `search:${this.hashString(query)}:${optionsHash}`;
  }
  
  private calculateCacheTTL(results: SearchResult[]): number {
    // æ ¹æ®ç»“æœè´¨é‡å’Œæ•°é‡åŠ¨æ€è®¡ç®—ç¼“å­˜æ—¶é—´
    const avgScore = results.reduce((sum, r) => sum + r.score, 0) / results.length;
    const baseTTL = 300000; // 5åˆ†é’Ÿ
    
    if (avgScore > 0.8) return baseTTL * 6; // 30åˆ†é’Ÿ
    if (avgScore > 0.6) return baseTL * 3; // 15åˆ†é’Ÿ
    return baseTTL; // 5åˆ†é’Ÿ
  }
  
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
  
  private hashString(str: string): string {
    // ç®€å•çš„å“ˆå¸Œå‡½æ•°
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      hash = ((hash << 5) - hash) + str.charCodeAt(i);
      hash |= 0; // è½¬æ¢ä¸º32ä½æ•´æ•°
    }
    return hash.toString(36);
  }
  
  private hashObject(obj: any): string {
    return this.hashString(JSON.stringify(obj));
  }
}

// ç±»å‹å®šä¹‰
interface SearchDocument {
  id: string;
  title: string;
  content: string;
  type: string;
  language?: string;
}

interface SearchResult {
  id: string;
  title: string;
  content: string;
  score: number;
  highlights: string[];
  metadata: Record<string, any>;
}

interface SearchResults {
  results: SearchResult[];
  totalCount: number;
  searchId: string;
  processingTime: number;
  cached: boolean;
}

interface SearchConfig {
  maxResults: number;
  minScoreThreshold: number;
  timeoutMs: number;
  fallbackToKeyword: boolean;
  hybridSearchRatio: number;
}

interface SearchOptions {
  limit?: number;
  minScore?: number;
  userPreferences?: any;
}

interface IndexResult {
  success: boolean;
  documentId: string;
  processingTime: number;
}

interface BatchIndexResult {
  successful: Array<{ documentId: string; result: IndexResult }>;
  failed: Array<{ documentId: string; error: string }>;
  total: number;
}

interface SystemMetrics {
  cacheStats: any;
  vectorDBStats: any;
  embeddingStats: any;
  performanceMetrics: any;
}
```

### 2. å¤šå› ç´ æ’åºç­–ç•¥é…ç½®

```typescript
// å¤šå› ç´ æ’åºç­–ç•¥é…ç½®
const rankingStrategies = {
  hybrid: {
    semanticWeight: 0.7,
    keywordWeight: 0.2,
    recencyWeight: 0.05,
    popularityWeight: 0.05,
    personalizationWeight: 0.0 // å¯åŸºäºç”¨æˆ·å†å²è°ƒæ•´
  },
  semanticOnly: {
    semanticWeight: 1.0,
    keywordWeight: 0.0,
    recencyWeight: 0.0,
    popularityWeight: 0.0
  },
  keywordOnly: {
    semanticWeight: 0.0,
    keywordWeight: 1.0,
    recencyWeight: 0.0,
    popularityWeight: 0.0
  },
  timeSensitive: {
    semanticWeight: 0.5,
    keywordWeight: 0.2,
    recencyWeight: 0.3,
    popularityWeight: 0.0
  }
};
```

### 3. æœ¬åœ°AIé›†æˆæ–¹æ¡ˆ

```python
# æœ¬åœ°æ¨¡å‹é›†æˆç­–ç•¥
class LocalAIIntegration:
    def __init__(self):
        self.smallModels = {
            "embedding": "all-MiniLM-L6-v2",  # 90MB
            "generation": "TinyLlama-1.1B",   # 1.2GB
            "summarization": "BART-large"     # 1.6GB
        }
        
    async def setupLocalModels(self):
        # æŒ‰éœ€ä¸‹è½½å’ŒåŠ è½½æ¨¡å‹
        for model_type, model_name in self.smallModels.items():
            if not self.hasModel(model_name):
                await self.downloadModel(model_name)
            self.loadModel(model_name)
```

### 4. æ™ºèƒ½æ¨èç®—æ³•

```typescript
// åŸºäºå¤šå› ç´ çš„æ¨èç®—æ³•
class SmartRecommendationEngine {
  async getRecommendations(noteId: string, options: RecommendationOptions) {
    const note = await getNote(noteId);
    
    // å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—
    const similarities = await Promise.all([
      this.calculateContentSimilarity(note),
      this.calculateSemanticSimilarity(note),
      this.calculateTemporalSimilarity(note),
      this.calculateUsageSimilarity(note)
    ]);
    
    // åŠ æƒç»¼åˆè¯„åˆ†
    const combinedScores = this.combineSimilarities(similarities, {
      contentWeight: 0.4,
      semanticWeight: 0.3,
      temporalWeight: 0.2,
      usageWeight: 0.1
    });
    
    return this.sortAndFilter(combinedScores, options.limit);
  }
}
```

---

## ğŸ› ï¸ å…·ä½“å®ç°æ­¥éª¤ä»£ç å‚è€ƒ

### 1. ç¯å¢ƒæ­å»º

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
npm install @llamaindex/core chroma-db sentence-transformers

# æˆ–ä½¿ç”¨Pythonåç«¯
pip install sentence-transformers chromadb llama-index
```

### 2. è¯­ä¹‰æœç´¢å‰ç«¯ç»„ä»¶

```typescript
// å‰ç«¯æœç´¢ç»„ä»¶
const SemanticSearchComponent = () => {
  const [results, setResults] = useState([]);
  
  const handleSearch = async (query: string) => {
    const response = await fetch('/api/semantic-search', {
      method: 'POST',
      body: JSON.stringify({ query })
    });
    setResults(await response.json());
  };
  
  return <SearchBox onSearch={handleSearch} results={results} />;
};
```

### 3. æœ¬åœ° AI åç«¯æœåŠ¡

```python
# åç«¯AIæœåŠ¡
@app.post("/api/generate-summary")
async def generate_summary(note_id: str):
    note_content = get_note_content(note_id)
    
    # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ‘˜è¦
    summary = local_ai_model.summarize(note_content)
    
    return {"summary": summary}
```

---

## ğŸ“Š æŠ€æœ¯é€‰å‹å®ç°å‚è€ƒ

### åµŒå…¥æ¨¡å‹é€‰æ‹©çŸ©é˜µ

```typescript
// åµŒå…¥æ¨¡å‹é…ç½®
const embeddingModels = {
  'all-MiniLM-L6-v2': {
    size: '90MB',
    speed: 5,
    quality: 3,
    memory: 5,
    scenario: 'ç§»åŠ¨ç«¯ã€èµ„æºå—é™ç¯å¢ƒ'
  },
  'all-mpnet-base-v2': {
    size: '420MB',
    speed: 4,
    quality: 5,
    memory: 4,
    scenario: 'å¹³è¡¡æ€§èƒ½å’Œè´¨é‡'
  },
  'text-embedding-3-small': {
    size: 'äº‘ç«¯',
    speed: 5,
    quality: 5,
    memory: 5,
    scenario: 'é«˜è´¨é‡è¦æ±‚åœºæ™¯'
  },
  'text-embedding-3-large': {
    size: 'äº‘ç«¯',
    speed: 4,
    quality: 6,
    memory: 4,
    scenario: 'æœ€é«˜è´¨é‡è¦æ±‚'
  },
  'embed-english-v3.0': {
    size: 'äº‘ç«¯',
    speed: 4,
    quality: 5,
    memory: 4,
    scenario: 'å¤šè¯­è¨€æ”¯æŒ'
  }
};
```

### å‘é‡æ•°æ®åº“é…ç½®

```typescript
// å‘é‡æ•°æ®åº“é…ç½®
const vectorDBConfigs = {
  chroma: {
    openSource: true,
    cloudService: true,
    distributed: false,
    memoryMode: 'æ··åˆ',
    queryPerformance: 4,
    learningCurve: 2
  },
  weaviate: {
    openSource: true,
    cloudService: true,
    distributed: true,
    memoryMode: 'å†…å­˜ä¼˜å…ˆ',
    queryPerformance: 5,
    learningCurve: 3
  },
  qdrant: {
    openSource: true,
    cloudService: true,
    distributed: true,
    memoryMode: 'æ··åˆ',
    queryPerformance: 5,
    learningCurve: 3
  },
  pinecone: {
    openSource: false,
    cloudService: true,
    distributed: true,
    memoryMode: 'äº‘ç«¯',
    queryPerformance: 5,
    learningCurve: 2
  },
  milvus: {
    openSource: true,
    cloudService: true,
    distributed: true,
    memoryMode: 'æ··åˆ',
    queryPerformance: 5,
    learningCurve: 4
  }
};
```

---

## ğŸ¯ å®æ–½ä¼˜å…ˆçº§ä»£ç å‚è€ƒ

### çŸ­æœŸç›®æ ‡å®ç°å‚è€ƒ

```typescript
// åŸºç¡€è¯­ä¹‰æœç´¢å®ç°
class BasicSemanticSearch {
  async setup() {
    // åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    this.embedder = await loadEmbeddingModel('all-MiniLM-L6-v2');
    
    // åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    this.vectorDB = new ChromaDB({
      path: './data/search_index',
      collectionName: 'notes'
    });
    
    console.log('åŸºç¡€è¯­ä¹‰æœç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ');
  }
}

// æ™ºèƒ½ç¬”è®°æ¨èå®ç°
class NoteRecommendation {
  async getSimilarNotes(noteId: string, limit = 5) {
    const note = await getNoteContent(noteId);
    const embedding = await this.embedder.embed(note.content);
    
    return await this.vectorDB.search({
      queryEmbedding: embedding,
      limit: limit,
      minScore: 0.6
    });
  }
}
```

### ä¸­æœŸç›®æ ‡å®ç°å‚è€ƒ

```typescript
// æœ¬åœ°AIæ¨¡å‹é›†æˆ
class LocalAIIntegration {
  async setupLocalModels() {
    // ä¸‹è½½å’Œé…ç½®æœ¬åœ°æ¨¡å‹
    await this.downloadModel('all-MiniLM-L6-v2');
    await this.downloadModel('TinyLlama-1.1B');
    
    // åˆå§‹åŒ–æœ¬åœ°æ¨ç†å¼•æ“
    this.embeddingEngine = new LocalEmbeddingEngine();
    this.generationEngine = new LocalGenerationEngine();
    
    console.log('æœ¬åœ°AIæ¨¡å‹é›†æˆå®Œæˆ');
  }
}

// é«˜çº§æœç´¢åŠŸèƒ½
class AdvancedSearch {
  async hybridSearch(query: string) {
    const [keywordResults, semanticResults] = await Promise.all([
      this.keywordSearch(query),
      this.semanticSearch(query)
    ]);
    
    return this.mergeResults(keywordResults, semanticResults);
  }
}
```

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

æœ¬æ–‡æ¡£åŒ…å« Obsidian 2025 åŠŸèƒ½åˆ†ææŠ¥å‘Šä¸­æ‰€æœ‰æŠ€æœ¯å®ç°ä»£ç ã€‚æ¯ä¸ªä»£ç å—éƒ½å¯¹åº”ä¸»æŠ¥å‘Šä¸­çš„å…·ä½“åŠŸèƒ½æ¨¡å—ï¼Œå¯ä»¥ç›´æ¥å‚è€ƒå®ç°æˆ–æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

### æ–‡ä»¶ç»“æ„è¯´æ˜
- **AIé›†æˆæŠ€æœ¯**: åŒ…å«AIåŸç”Ÿé›†æˆã€å®æ—¶å»ºè®®å¼•æ“ç­‰æ ¸å¿ƒåŠŸèƒ½
- **æ’ä»¶æŠ€æœ¯å®ç°**: åŒ…å«å„çƒ­é—¨æ’ä»¶çš„å®Œæ•´æŠ€æœ¯æ¶æ„
- **æ ¸å¿ƒæœç´¢ç³»ç»Ÿ**: ä¼ä¸šçº§è¯­ä¹‰æœç´¢ç³»ç»Ÿçš„å®Œæ•´å®ç°
- **æŠ€æœ¯é€‰å‹å‚è€ƒ**: å„ç§æŠ€æœ¯ç»„ä»¶çš„é…ç½®å’Œæ¯”è¾ƒ
- **å®æ–½ä¼˜å…ˆçº§**: åˆ†é˜¶æ®µå®æ–½çš„å…·ä½“ä»£ç å‚è€ƒ

### å¼€å‘å»ºè®®
1. æ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æŠ€æœ¯ç»„ä»¶
2. å‚è€ƒä»£ç å®ç°è¿›è¡Œå®šåˆ¶åŒ–å¼€å‘
3. æ³¨æ„æ€§èƒ½ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
4. éµå¾ªéšç§ä¿æŠ¤å’Œæ•°æ®å®‰å…¨